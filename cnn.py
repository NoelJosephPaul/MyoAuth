import numpy as np
import io
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# Load the dataset
data = """Username,Features
nikhil,"[42.09159624413146, 12580.95, 0.5, 0.06828312067067656, 0.05735413590417293, 53.4249635474555, 607950.2935, 442.983314607804, 94355.44601146225, 129493412.51550001, 60, -1.9278054213477902e-17, -942467.5380562044]"
nikhil,"[48.066775700934585, 14044.24, 0.460093896713615, 0.02431214166018411, -0.07132400247097248, 59.576768778084826, 759569.7548999999, 485.81952486615205, 103965.37832135653, 162547927.5485999, 61, -0.00021463876109817175, -1049820.1842374792]"
nikhil,"[40.183427230046945, 10031.97, 0.3867924528301887, -0.12637943183880712, 2.8486332831632195, 53.909709809431256, 619032.7008999999, 446.6540205649677, 95137.30638033812, 131853965.2917, 46, 0.0, -953293.4524618923]"
nikhil,"[43.65953051643192, 11180.54, 0.41509433962264153, -0.005001446366815094, 0.2862009385848876, 56.69192078796685, 684576.437, 481.713898354885, 102605.0603495905, 145814781.08100003, 42, 0.0, -1027055.8973416795]"
nikhil,"[42.74206572769954, 11459.130000000001, 0.4339622641509434, 0.005167727895128643, 0.33644009596089886, 54.12638943992862, 624018.8652, 449.6468907408679, 95774.78772780485, 132916018.28759998, 170, -1.8992361629821462e-17, -957992.2605534222]"
niya,"[63.971915887850464, 17058.39, 0.431924882629108, 0.032978403727827704, -0.2524321329655437, 77.92262318012034, 1299394.1335, 655.7866085848675, 140338.33423716165, 278070344.5689998, 42, -0.00012402170864151646, -1473361.2232692717]"
niya,"[59.77257009345795, 15243.96, 0.4460093896713615, -0.020568630965404157, 0.4418538571037298, 74.90270218039689, 1200628.7658999998, 655.1673932997844, 140205.82216615387, 256934555.90259984, 176, -0.00022071123382688874, -1459940.1074344916]"
niya,"[70.1679716981132, 18673.99, 0.4218009478672986, 0.022257520998076045, -0.7566511103466569, 84.53797410932813, 1515093.8421, 739.5057229843792, 156775.2132726884, 321199894.5252, 36, -0.00011369782013305147, -1656397.2029973934]"
niya,"[60.30981220657276, 14884.070000000002, 0.41509433962264153, 0.08036929521710996, -0.43622331205845244, 72.89306737839331, 1131754.0449, 664.8002054992724, 141602.44377134502, 241063611.56369996, 44, -2.569149733719428e-17, -1453084.8570010294]"
niya,"[65.48765258215964, 17009.39, 0.4339622641509434, -0.09822610504319677, 0.3533339439469243, 81.2186659117734, 1405048.4705, 689.5992409796495, 146884.63832866534, 299275324.2165, 38, -1.2383796047315254e-17, -1556229.20522519]"
noel,"[29.33657276995305, 7845.75, 0.42924528301886794, 0.02476657075717524, 0.33176422425782404, 37.35090442683212, 297154.1831, 315.66920266773553, 67237.54016822767, 63293841.000300005, 170, 0.0, -628796.9566011876]"
noel,"[31.795801886792454, 8031.789999999999, 0.41706161137440756, 0.021743655848819453, 0.6165865342826207, 40.467492247155434, 347175.0009, 319.4915327009221, 67732.20493259549, 73601100.1908, 42, -0.00014697587373546964, -649968.0886176564]"
noel,"[46.03981220657277, 11857.43, 0.4056603773584906, 0.06111943058346034, 0.7977999855773863, 58.2803467707452, 723475.5486000001, 468.4559611895716, 99781.11973337876, 154100291.85179996, 170, 0.0, -1014580.4774348078]"
noel,"[51.705070422535215, 14262.470000000001, 0.4339622641509434, 0.026630967056205178, 0.9668143356347185, 68.26471089114524, 992595.0703999999, 553.2201925339281, 117835.90100972667, 211422749.99519998, 43, -1.543663168829769e-17, -1217411.8291218504]"
noel,"[57.00657276995306, 15802.55, 0.4339622641509434, -0.10018382654305424, 1.423450584415006, 78.84069381315062, 1323977.1152000001, 643.8569655740788, 137141.5336672788, 282007125.53760004, 43, 0.0, -1447470.7871423415]"
sanjana,"[57.71733644859813, 15805.920000000002, 0.4413145539906103, -0.04522378251895726, 0.3512848373392421, 74.60203551694168, 1191009.2325, 636.3537214072554, 136179.69638115267, 254875975.75499988, 165, -0.0001293511475506396, -1433508.3674630541]"
sanjana,"[37.51713615023474, 10355.810000000001, 0.4009433962264151, -0.047207090780869806, 0.41183521490255703, 50.014673128683086, 532812.5835, 460.048511432922, 97990.33293521238, 113489080.28549999, 48, 0.0, -957565.425772655]"
sanjana,"[38.90328638497653, 10458.7, 0.4528301886792453, 0.0036274154385132957, 0.7856021983251553, 50.898581503427316, 551811.7726, 447.53175541676336, 95324.2639037706, 117535907.56379998, 54, 1.9082123785210846e-17, -942217.3299866116]"
sanjana,"[46.274600938967126, 12412.33, 0.4009433962264151, -0.006841369005778389, 0.4612146524246463, 59.57350448890179, 755937.5190999999, 545.7625115369435, 116247.41495736898, 161014691.5683, 57, 3.1295137258973515e-17, -1169567.3922338733]"
sanjana,"[45.14652582159625, 11149.259999999998, 0.3915094339622642, -0.20351933836355382, 0.5147313791862604, 58.998801360466906, 741422.8737000001, 534.6062012064597, 113871.1208569759, 157923072.0981, 43, 1.597410642712948e-17, -1134828.2685067775]"
"""

# Assuming you have loaded your data into a pandas DataFrame named 'df'
df = pd.read_csv(io.StringIO(data))

# Group by username and split the data
train_df, test_df = [], []
for username, group in df.groupby('Username'):
    # Select 4 random samples for training
    train_data = group.sample(n=4, random_state=42)
    # Remaining one sample for testing
    test_data = group.drop(train_data.index)

    train_df.append(train_data)
    test_df.append(test_data)

# Combine the dataframes for training and testing
train_df = pd.concat(train_df)
test_df = pd.concat(test_df)

# Extract features and labels
X_train = np.array([eval(features) for features in train_df['Features']])
y_train = train_df['Username'].astype('category').cat.codes.values

X_test = np.array([eval(features) for features in test_df['Features']])
y_test = test_df['Username'].astype('category').cat.codes.values

# Standardize the input features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print(y_test)
# Reshape the data for CNN input
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Build the CNN model
model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(len(np.unique(y_train)), activation='softmax'))

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=1000, batch_size=16, validation_data=(X_test, y_test))
model.save("model.h5")
